{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/attarmau/VisionGuard-Anomaly-Detection-for-B2B-Sensors/blob/main/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7rH452ZP3Zp"
      },
      "source": [
        "## Anomaly Detection with FiftyOne and Anomalib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1vFAPSGm5O5EwP8n1l34tB5LG-ofrLe_c\">"
      ],
      "metadata": {
        "id": "JzYZVKsQQBwl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qaklu9HDP3Zq"
      },
      "source": [
        "Anomaly detection (AD) is a crucial task in mission-critical applications such as fraud detection, network security, and medical diagnosis. Anomaly detection on *visual* data like images, videos, and satellite imagery, is particularly challenging task due to the high dimensionality of the data and the complexity of the underlying patterns. Yet visual anomaly detection is essential for detecting defects in manufacturing, identifying suspicious activity in surveillance footage, and detecting abnormalities in medical images.\n",
        "\n",
        "In this walkthrough, you'll learn how to perform anomaly detection on visual data using [FiftyOne](https://fiftyone.ai/) and [Anomalib](https://github.com/openvinotoolkit/anomalib) from the OpenVINOâ„¢ toolkit. We'll use the [MVTec AD dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad) for demonstration, which contains images of various objects with anomalies like scratches, dents, and holes.\n",
        "\n",
        "The notebook covers the following:\n",
        "\n",
        "- Loading the MVTec AD dataset in FiftyOne\n",
        "- Training an anomaly detection model with Anomalib\n",
        "- Evaluating anomaly detection models in FiftyOne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAFVPw-0P3Zq"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8v0QBT3P3Zr"
      },
      "source": [
        "### Installing dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQQSOtXCP3Zr"
      },
      "source": [
        "Make sure you are running this in a virtual environment with `python=3.10`.\n",
        "\n",
        "```bash\n",
        "conda create -n anomalib_env python=3.10; conda activate anomalib_env\n",
        "```\n",
        "\n",
        "Anomalib requires Python 3.10, so make sure you have the correct version installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcy4ptQUP3Zr"
      },
      "source": [
        "After this, install Anomalib and its dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7DlL1mcP3Zr"
      },
      "outputs": [],
      "source": [
        "!pip install -U torchvision einops FrEIA timm open_clip_torch imgaug lightning kornia openvino git+https://github.com/openvinotoolkit/anomalib.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKdAKyt7P3Zr"
      },
      "source": [
        "Install Anomalib from source, per the instructions in the [Anomalib README](https://github.com/openvinotoolkit/anomalib?tab=readme-ov-file#-installation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEprmOUwP3Zr"
      },
      "source": [
        "Install the latest version of FiftyOne from source to use the most recent version of the Hugging Face Hub integration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNkfn74SP3Zr"
      },
      "outputs": [],
      "source": [
        "!pip install -U git+https://github.com/voxel51/fiftyone.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6thhJS32P3Zr"
      },
      "source": [
        "Just a few more packages to install, and we're ready to go. Now you can see why we recommend using a virtual environment for this project!\n",
        "\n",
        "- `huggingface_hub` for loading the MVTec AD dataset\n",
        "- `clip` for computing image embeddings\n",
        "- `umap-learn` for dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8hoGR4SP3Zs"
      },
      "outputs": [],
      "source": [
        "!pip install -U huggingface_hub umap-learn git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7HxyuTdP3Zs"
      },
      "source": [
        "### Loading and Visualizing the MVTec AD dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDPbfPrDP3Zs"
      },
      "source": [
        "Now let's import all of the relevant modules we will need from FiftyOne:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBfqbJ6UP3Zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b7eb15-638b-4a0a-90d8-4d8187942506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Migrating database to v0.23.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.migrations.runner:Migrating database to v0.23.8\n"
          ]
        }
      ],
      "source": [
        "import fiftyone as fo # base library and app\n",
        "import fiftyone.brain as fob # ML methods\n",
        "import fiftyone.zoo as foz # zoo datasets and models\n",
        "from fiftyone import ViewField as F # helper for defining views\n",
        "import fiftyone.utils.huggingface as fouh # Hugging Face integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg2LfMAyP3Zs"
      },
      "source": [
        "And load the MVTec AD dataset from the Hugging Face Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IukZwWk8P3Zs"
      },
      "outputs": [],
      "source": [
        "dataset = fouh.load_from_hub(\"Voxel51/mvtec-ad\", persistent=True, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OySI-xKDP3Zs"
      },
      "source": [
        "ðŸ’¡ It is also possible to load the MVTec AD data directly from Anomalib:\n",
        "\n",
        "```python\n",
        "from anomalib.data import MVTec\n",
        "datamodule = MVTec()\n",
        "```\n",
        "\n",
        "But this way we have all of the metadata and annotations in FiftyOne, which is useful for visualization and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d87MXEuoP3Zs"
      },
      "source": [
        "Before moving on, let's take a look at the dataset in the [FiftyOne App](https://docs.voxel51.com/user_guide/app.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgJQIJDWP3Zs"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A62REdSP3Zs"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1CDKbMX_sERcER_KfZdNiq5pyrPcqF8kb\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxgWfgRdP3Zs"
      },
      "source": [
        "The dataset has 5354 images across 12 object categories. Each category has \"good\" images and \"anomalous\" images with defects like scratches, dents, and holes. Each of the anomalous samples also has a mask which localizes the defective regions of the image.\n",
        "\n",
        "The defect labels differ across categories, which is typical in real-world anomaly detection scenarios. In typical anomaly detection scenarios, you train a different model for each category. Here we'll go through the process for one category, and you can apply the same steps to other categories.\n",
        "\n",
        "One more thing to note is that the dataset is split into training and test sets. The training set contains only \"good\" images, while the test set contains both \"good\" and \"anomalous\" images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku6_fz4EP3Zs"
      },
      "source": [
        "Before we train a model, let's dig into the dataset a bit more. We can get a feel for the structure and patterns hidden in our data by computing image embeddings and visualizing them in a lower-dimensional space. First, we'll compute embeddings for all the images in the dataset using the CLIP model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE5LzwwAP3Zs"
      },
      "outputs": [],
      "source": [
        "model = foz.load_zoo_model(\"clip-vit-base32-torch\")  # load the CLIP model from the zoo\n",
        "\n",
        "# Compute embeddings for the dataset\n",
        "dataset.compute_embeddings(\n",
        "    model=model, embeddings_field=\"clip_embeddings\", batch_size=64\n",
        ")\n",
        "\n",
        "# Dimensionality reduction using UMAP on the embeddings\n",
        "fob.compute_visualization(\n",
        "    dataset, embeddings=\"clip_embeddings\", method=\"umap\", brain_key=\"clip_vis\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhn73_kJP3Zt"
      },
      "source": [
        "Refresh the FiftyOne App, click the \"+\" tab, and select \"Embeddings\". Choose \"all_clip_vis\" from the dropdown menu. You'll see a scatter plot of the image embeddings in a 2D space, where each point corresponds to a sample in the dataset. Using the color-by dropdown, notice how the embeddings cluster based on the object category. This is because CLIP encodes semantic information about the images. Also note that *within* a category, CLIP embeddings don't cluster based on the defect type."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1la7NMj61o6HZeutNqRakDRSZpBW_1c17\">"
      ],
      "metadata": {
        "id": "_mgdvyarQ5ri"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o-ocpX9P3Zt"
      },
      "source": [
        "If instead we embed our images using a traditional computer vision model like ResNet, we also see some clustering within a category based on the defect type. However, as we established earlier, we will not have access to defect labels during inference. Instead, we'll use an unsupervised anomaly detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uUX8tj1P3Zt"
      },
      "outputs": [],
      "source": [
        "model = foz.load_zoo_model(\"resnet50-imagenet-torch\")  # load the ResNet50 model from the zoo\n",
        "\n",
        "# Compute embeddings for the dataset â€” this might take a while on a CPU\n",
        "dataset.compute_embeddings(\n",
        "    model=model, embeddings_field=\"resnet50_embeddings\"\n",
        ")\n",
        "\n",
        "# Dimensionality reduction using UMAP on the embeddings\n",
        "fob.compute_visualization(\n",
        "    dataset, embeddings=\"resnet50_embeddings\", method=\"umap\", brain_key=\"resnet50_vis\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7a8eF6SP3Zt"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1WmRLENimEw5Ztkjihe4V4ZJhqgFM4ZYM\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv0-gvTGP3Zt"
      },
      "source": [
        "ðŸ’¡ For deep dives into embeddings and dimensionality reduction, check out our tutorials:\n",
        "\n",
        "- [Using Image Embeddings](https://docs.voxel51.com/tutorials/image_embeddings.html)\n",
        "- [Visualizing Data with Dimensionality Reduction Techniques](https://docs.voxel51.com/tutorials/dimension_reduction.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6A8nblZP3Zt"
      },
      "source": [
        "## Training an Anomaly Detection Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZKHL3vlP3Zt"
      },
      "source": [
        "Now that we have a sense of the dataset, we're ready to train an anomaly detection model using Anomalib.\n",
        "\n",
        "**Task**: Anomalib supports classification, detection, and segmentation tasks for images. We'll focus on segmentation, where the model predicts whether each pixel in the image is anomalous or not, creating a mask that localizes the defect.\n",
        "\n",
        "**Model**: Anomalib supports a variety of anomaly detection algorithms, including [Deep Feature Kernel Density Estimation (DFKDE)](https://anomalib.readthedocs.io/en/v1.0.1/markdown/guides/reference/models/image/dfkde.html), [FastFlow](https://arxiv.org/abs/2111.07677), and [Reverse Distillation](https://arxiv.org/abs/2201.10703v2). For a complete list of supported algorithms, check out [Anomalib's reference guide](https://anomalib.readthedocs.io/en/v1.0.1/markdown/guides/reference/models/image/index.html). For this walkthrough, we'll use two algorithms:\n",
        "\n",
        "- [PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization](https://arxiv.org/abs/2011.08785)\n",
        "- [PatchCore: Towards Total Recall in Industrial Anomaly Detection](https://arxiv.org/abs/2106.08265)\n",
        "\n",
        "**Preprocessing**: For this walkthrough, we will resize the images to 256x256 pixels before training the model. Adding this as a transform via Torchvision's `Resize` class lets us resize the images on-the-fly during training and inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo9mfmLFP3Zt"
      },
      "source": [
        "Import the necessary modules from Anomalib and helper modules for processing images and paths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbJTqvFzP3Zt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import shutil\n",
        "from torchvision.transforms.v2 import Resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSlSG58zP3Zu"
      },
      "outputs": [],
      "source": [
        "from anomalib import TaskType\n",
        "from anomalib.data.image.folder import Folder\n",
        "from anomalib.deploy import ExportType, OpenVINOInferencer\n",
        "from anomalib.engine import Engine\n",
        "from anomalib.models import Padim, Patchcore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEgliRQkP3Zu"
      },
      "source": [
        "Now define some constants to use throughout the notebook.\n",
        "\n",
        "- `OBJECT`: The object category we'll focus on. For this walkthrough, we'll use \"bottle\". If you want to loop over categories, you can get the list of categories from the dataset with `dataset.distinct(\"category.label\")`.\n",
        "- `ROOT_DIR`: The root directory where Anomalib will look for images and masks. Our data is already stored on disk, so we will just symlink files to the directory Anomalib expects.\n",
        "- `TASK`: The task we're performing. We'll use \"segmentation\" for this walkthrough.\n",
        "- `IMAGE_SIZE`: The size to resize images to before training the model. We'll use $256$ x $256$ pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRKFiOqjP3Zu"
      },
      "outputs": [],
      "source": [
        "OBJECT = \"bottle\" ## object to train on\n",
        "ROOT_DIR = Path(\"/content/mvtec_ad\") ## root directory to store data for anomalib\n",
        "TASK = TaskType.SEGMENTATION ## task type for the model\n",
        "IMAGE_SIZE = (256, 256) ## preprocess image size for uniformity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAr1njSZP3Zu"
      },
      "source": [
        "For a given object type (category), the `create_datamodule()` function below creates an Anomalib  `DataModule` object. This will get passed into our engine's `fit()` method to train the model, and will be used to instantiate dataloaders for training and validation.\n",
        "\n",
        "The code might look complex, so let's break down what's going on:\n",
        "\n",
        "- We create subsets of our data containing only the \"good\" training images and \"anomalous\" images for validation.\n",
        "- We symlink the images and masks to the directory Anomalib expects.\n",
        "- We instantiate and setup a datamodule from Anomalib's `Folder`, which is the general-purpose class for custom datasets.\n",
        "\n",
        "ðŸ’¡ It is also possible to create a torch `DataLoader` from scratch and pass it to the engine's `fit()` method. This gives you more control over the data loading process. This is left as an exercise for the reader ðŸ˜‰."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsBZGKLXP3Zu"
      },
      "outputs": [],
      "source": [
        "def create_datamodule(object_type, transform=None):\n",
        "    ## Build transform\n",
        "    if transform is None:\n",
        "        transform = Resize(IMAGE_SIZE, antialias=True)\n",
        "\n",
        "    normal_data = dataset.match(F(\"category.label\") == object_type).match(\n",
        "        F(\"split\") == \"train\"\n",
        "    )\n",
        "    abnormal_data = (\n",
        "        dataset.match(F(\"category.label\") == object_type)\n",
        "        .match(F(\"split\") == \"test\")\n",
        "        .match(F(\"defect.label\") != \"good\")\n",
        "    )\n",
        "\n",
        "    normal_dir = Path(ROOT_DIR) / object_type / \"normal\"\n",
        "    abnormal_dir = ROOT_DIR / object_type / \"abnormal\"\n",
        "    mask_dir = ROOT_DIR / object_type / \"mask\"\n",
        "\n",
        "    # create directories if they do not exist\n",
        "    os.makedirs(normal_dir, exist_ok=True)\n",
        "    os.makedirs(abnormal_dir, exist_ok=True)\n",
        "    os.makedirs(mask_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(str(normal_dir)):\n",
        "        normal_data.export(\n",
        "            export_dir=str(normal_dir),\n",
        "            dataset_type=fo.types.ImageDirectory,\n",
        "            export_media=True, ### Can use \"symlink\" if local\n",
        "        )\n",
        "\n",
        "    for sample in abnormal_data.iter_samples():\n",
        "        base_filename = sample.filename\n",
        "        dir_name = os.path.dirname(sample.filepath).split(\"/\")[-1]\n",
        "        new_filename = f\"{dir_name}_{base_filename}\"\n",
        "        if not os.path.exists(str(abnormal_dir / new_filename)):\n",
        "            shutil.copyfile(sample.filepath, str(abnormal_dir / new_filename))\n",
        "            # os.symlink(sample.filepath, str(abnormal_dir / new_filename)) # Can symlink if local\n",
        "\n",
        "        if not os.path.exists(str(mask_dir / new_filename)):\n",
        "            shutil.copyfile(sample.defect_mask.mask_path, str(mask_dir / new_filename))\n",
        "            os.symlink(sample.defect_mask.mask_path, str(mask_dir / new_filename)) # Can symlink if local\n",
        "\n",
        "    datamodule = Folder(\n",
        "        name=object_type,\n",
        "        root=ROOT_DIR,\n",
        "        normal_dir=normal_dir,\n",
        "        abnormal_dir=abnormal_dir,\n",
        "        mask_dir=mask_dir,\n",
        "        task=TASK,\n",
        "        transform=transform\n",
        "    )\n",
        "    datamodule.setup()\n",
        "    return datamodule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSmVK8FFP3Zu"
      },
      "source": [
        "Now we can put it all together. The `train_and_export_model()` function below trains an anomaly detection model using Anomalib's `Engine` class, exports the model to OpenVINO, and returns the model \"inferencer\" object. The inferencer object is used to make predictions on new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbzI5Pv6P3Zu"
      },
      "outputs": [],
      "source": [
        "def train_and_export_model(object_type, model, transform=None):\n",
        "    engine = Engine(task=TASK)\n",
        "    datamodule = create_datamodule(object_type, transform=transform)\n",
        "    engine.fit(model=model, datamodule=datamodule)\n",
        "\n",
        "    engine.export(\n",
        "        model=model,\n",
        "        export_type=ExportType.OPENVINO,\n",
        "    )\n",
        "    output_path = Path(engine.trainer.default_root_dir)\n",
        "\n",
        "\n",
        "    openvino_model_path = output_path / \"weights\" / \"openvino\" / \"model.bin\"\n",
        "    metadata = output_path / \"weights\" / \"openvino\" / \"metadata.json\"\n",
        "\n",
        "    inferencer = OpenVINOInferencer(\n",
        "        path=openvino_model_path,\n",
        "        metadata=metadata,\n",
        "        device=\"CPU\",\n",
        "    )\n",
        "    return inferencer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF0Lfrm0P3Zu"
      },
      "source": [
        "Let's try this with `PaDiM` first. The training process should take less than a minute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJeAsVTbP3Zv"
      },
      "outputs": [],
      "source": [
        "model = Padim()\n",
        "\n",
        "inferencer = train_and_export_model(OBJECT, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXBHi3yP3Zv"
      },
      "source": [
        "And just like that, we have an anomaly detection model trained on the \"bottle\" category. Let's run our inferencer on a single image and inspect the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt9-d8EpP3Zv",
        "outputId": "d69d75c7-d84d-4546-eead-984ef6acac6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ImageResult(image=[[[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]], pred_score=0.7751642969087686, pred_label=1, anomaly_map=[[0.32784402 0.32784402 0.32784414 ... 0.3314721  0.33147204 0.33147204]\n",
            " [0.32784402 0.32784402 0.32784414 ... 0.3314721  0.33147204 0.33147204]\n",
            " [0.32784408 0.32784408 0.3278442  ... 0.33147222 0.33147216 0.33147216]\n",
            " ...\n",
            " [0.32959    0.32959    0.32959005 ... 0.3336093  0.3336093  0.3336093 ]\n",
            " [0.3295899  0.3295899  0.32958996 ... 0.33360928 0.33360928 0.33360928]\n",
            " [0.3295899  0.3295899  0.32958996 ... 0.33360928 0.33360928 0.33360928]], gt_mask=None, gt_boxes=None, pred_boxes=None, box_labels=None, pred_mask=[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]], heat_map=[[[153 235 255]\n",
            "  [153 235 255]\n",
            "  [153 235 255]\n",
            "  ...\n",
            "  [153 236 255]\n",
            "  [153 236 255]\n",
            "  [153 236 255]]\n",
            "\n",
            " [[153 235 255]\n",
            "  [153 235 255]\n",
            "  [153 235 255]\n",
            "  ...\n",
            "  [153 236 255]\n",
            "  [153 236 255]\n",
            "  [153 236 255]]\n",
            "\n",
            " [[153 235 255]\n",
            "  [153 235 255]\n",
            "  [153 235 255]\n",
            "  ...\n",
            "  [153 236 255]\n",
            "  [153 236 255]\n",
            "  [153 236 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[153 236 255]\n",
            "  [153 236 255]\n",
            "  [153 236 255]\n",
            "  ...\n",
            "  [153 238 255]\n",
            "  [153 238 255]\n",
            "  [153 238 255]]\n",
            "\n",
            " [[153 236 255]\n",
            "  [153 236 255]\n",
            "  [153 236 255]\n",
            "  ...\n",
            "  [153 238 255]\n",
            "  [153 238 255]\n",
            "  [153 238 255]]\n",
            "\n",
            " [[153 236 255]\n",
            "  [153 236 255]\n",
            "  [153 236 255]\n",
            "  ...\n",
            "  [153 238 255]\n",
            "  [153 238 255]\n",
            "  [153 238 255]]], segmentations=[[[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]])\n"
          ]
        }
      ],
      "source": [
        "## get the test split of the dataset\n",
        "test_split = dataset.match(F(\"category.label\") == OBJECT).match(F(\"split\") == \"test\")\n",
        "\n",
        "## get the first sample from the test split\n",
        "test_image = Image.open(test_split.first().filepath)\n",
        "\n",
        "output = inferencer.predict(image=test_image)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8M9LqZCP3Zv"
      },
      "source": [
        "The output contains a scalar anomaly score `pred_score`, a `pred_mask` denoting the predicted anomalous regions, and a heatmap `anomaly_map` showing the anomaly scores for each pixel. This is all valuable information for understanding the model's predictions. The `run_inference()` function below will take a FiftyOne sample collection (e.g. our test set) as input, along with the inferencer object, and a key for storing the results in the samples. It will run the model on each sample in the collection and store the results. The `threshold` argument acts as a cutoff for the anomaly score. If the score is above the threshold, the sample is considered anomalous. In this example, we'll use a threshold of $0.5$, but you can experiment with different values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYSrg-j9P3Zv"
      },
      "outputs": [],
      "source": [
        "def run_inference(sample_collection, inferencer, key, threshold=0.5):\n",
        "    for sample in sample_collection.iter_samples(autosave=True, progress=True):\n",
        "        output = inferencer.predict(image=Image.open(sample.filepath))\n",
        "\n",
        "        conf = output.pred_score\n",
        "        anomaly = \"normal\" if conf < threshold else \"anomaly\"\n",
        "\n",
        "        sample[f\"pred_anomaly_score_{key}\"] = conf\n",
        "        sample[f\"pred_anomaly_{key}\"] = fo.Classification(label=anomaly)\n",
        "        sample[f\"pred_anomaly_map_{key}\"] = fo.Heatmap(map=output.anomaly_map)\n",
        "        sample[f\"pred_defect_mask_{key}\"] = fo.Segmentation(mask=output.pred_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dnQ6awqP3Zv"
      },
      "outputs": [],
      "source": [
        "run_inference(test_split, inferencer, \"padim\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTJ3cP8HP3Zv"
      },
      "source": [
        "Let's visualize these results in the FiftyOne App:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQnpPg7CP3Zv"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(view=test_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1jodt80UlRp7A3wMQvoS0jQQHi2_w6wl7\">"
      ],
      "metadata": {
        "id": "WQiGBBqIRYCJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXd2zRfUP3Zv"
      },
      "source": [
        "## Evaluating Anomaly Detection Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Rj6gJmP3Zv"
      },
      "source": [
        "We have an anomaly detection model, but how do we know if it's any good? For one, we can evaluate the model using metrics like precision, recall, and F1 score. FiftyOne's [Evaluation API](https://docs.voxel51.com/user_guide/evaluation.html#) makes this easy. We are going to evaluate the full-image classification performance of the model, as well as the segmentation performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB0UHNtiP3Zv"
      },
      "source": [
        "We need to prepare our data for evaluation. First, we need to add null masks for the \"normal\" images to ensure the evaluation is fair:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeyNz_EBP3Zv"
      },
      "outputs": [],
      "source": [
        "for sample in test_split.iter_samples(autosave=True, progress=True):\n",
        "    if sample[\"defect\"].label == \"good\":\n",
        "        sample[\"defect_mask\"] = fo.Segmentation(mask=np.zeros_like(sample[\"pred_defect_mask_padim\"].mask))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "636j8-WWP3Zw"
      },
      "source": [
        "We also need to ensure consistency in naming/labels between ground truth and predictions. We'll rename all of our \"good\" images to \"normal\" and every type of anomaly to \"anomaly\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ormb7-oUP3Zw"
      },
      "outputs": [],
      "source": [
        "old_labels = test_split.distinct(\"defect.label\")\n",
        "label_map = {label:\"anomaly\" for label in old_labels if label != \"good\"}\n",
        "label_map[\"good\"] = \"normal\"\n",
        "mapped_view = test_split.map_labels(\"defect\", label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb0HQoSqP3Zw"
      },
      "outputs": [],
      "source": [
        "session.view = mapped_view.view()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1490VjnzDy1M5Af41gLGov3JCxG5jpYuq\">"
      ],
      "metadata": {
        "id": "HEBqUNr-RnAp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irqqooIeP3Zw"
      },
      "source": [
        "For classification, we'll use binary evaluation, with \"normal\" as the negative class and \"anomaly\" as the positive class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsrFGQfVP3Zw"
      },
      "outputs": [],
      "source": [
        "eval_classif_padim = mapped_view.evaluate_classifications(\n",
        "    \"pred_anomaly_padim\",\n",
        "    gt_field=\"defect\",\n",
        "    eval_key=\"eval_classif_padim\",\n",
        "    method=\"binary\",\n",
        "    classes=[\"normal\", \"anomaly\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irTis_kRP3Zw",
        "outputId": "d47b2013-9296-48a4-8ec2-3a245b60fd63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.95      0.90      0.92        20\n",
            "     anomaly       0.97      0.98      0.98        63\n",
            "\n",
            "    accuracy                           0.96        83\n",
            "   macro avg       0.96      0.94      0.95        83\n",
            "weighted avg       0.96      0.96      0.96        83\n",
            "\n"
          ]
        }
      ],
      "source": [
        "eval_classif_padim.print_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_N-VVz9P3Zw"
      },
      "source": [
        "The model performs quite well on the classification task!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKyMNHb4P3Zw"
      },
      "source": [
        "If we go back over to the app and sort by anomaly score, we can see that certain types of anomalies tend to have higher scores than others. In this example, `contamination` instances tend to have either very high or very low scores relative to `broken_small` and `broken_large`. When we put this model in production we might be more likely to miss certain types of anomalies. Other types of models, or ensembles of models, might be more robust to this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB-rJDgjP3Zw"
      },
      "source": [
        "For segmentation evaluation, we will only be interested in pixel values of $0$ (normal) and $255$ (anomaly), so we will filter our report for these \"classes\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMgeUHldP3Zw"
      },
      "outputs": [],
      "source": [
        "eval_seg_padim = mapped_view.evaluate_segmentations(\n",
        "    \"pred_defect_mask_padim\",\n",
        "    gt_field=\"defect_mask\",\n",
        "    eval_key=\"eval_seg_padim\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM9Mm4qzP3Zw",
        "outputId": "5555d6ae-ffc0-42a4-c7c7-a34dc74aed79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98 63343269.0\n",
            "         255       0.60      0.89      0.72 3886731.0\n",
            "\n",
            "   micro avg       0.96      0.96      0.96 67230000.0\n",
            "   macro avg       0.80      0.93      0.85 67230000.0\n",
            "weighted avg       0.97      0.96      0.96 67230000.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "eval_seg_padim.print_report(classes=[0, 255])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJX87jDuP3Zx"
      },
      "source": [
        "### Comparing Anomaly Detection Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RblZT8XP3Zx"
      },
      "source": [
        "Just because anomaly detection is unsupervised doesn't mean we can't compare models and choose the best one for our use case. We can train multiple models on the same data and compare their performance using metrics like F1 score, precision, and recall. We can also compare the models visually by inspecting the masks and heatmaps they generate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCNSMM3xP3Zx"
      },
      "source": [
        "Let's repeat the training process for the `PatchCore` model and compare the two models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JT5Ftv1P3Zx"
      },
      "outputs": [],
      "source": [
        "## Train Patchcore model and run inference\n",
        "\n",
        "model = Patchcore()\n",
        "\n",
        "## This will take a little longer to train, but should still be < 5 minutes\n",
        "inferencer = train_and_export_model(OBJECT, model)\n",
        "\n",
        "run_inference(mapped_view, inferencer, \"patchcore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iElDpHTYP3Zx",
        "outputId": "aaa080d5-2fff-4ca6-a84f-c01c41d1d1d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.95      1.00      0.98        20\n",
            "     anomaly       1.00      0.98      0.99        63\n",
            "\n",
            "    accuracy                           0.99        83\n",
            "   macro avg       0.98      0.99      0.98        83\n",
            "weighted avg       0.99      0.99      0.99        83\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Evaluate Patchcore model on classification task\n",
        "eval_classif_patchcore = mapped_view.evaluate_classifications(\n",
        "    \"pred_anomaly_patchcore\",\n",
        "    gt_field=\"defect\",\n",
        "    eval_key=\"eval_classif_patchcore\",\n",
        "    method=\"binary\",\n",
        "    classes=[\"normal\", \"anomaly\"],\n",
        ")\n",
        "\n",
        "eval_classif_patchcore.print_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIztc0pRP3Zx"
      },
      "outputs": [],
      "source": [
        "eval_seg_patchcore = mapped_view.match(F(\"defect.label\") == \"anomaly\").evaluate_segmentations(\n",
        "    \"pred_defect_mask_patchcore\",\n",
        "    gt_field=\"defect_mask\",\n",
        "    eval_key=\"eval_seg_patchcore\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWQ_8kErP3Zx",
        "outputId": "40c25f51-ad53-439b-a938-ed64e1bdb15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97 47143269.0\n",
            "         255       0.60      0.85      0.70 3886731.0\n",
            "\n",
            "   micro avg       0.95      0.95      0.95 51030000.0\n",
            "   macro avg       0.80      0.90      0.84 51030000.0\n",
            "weighted avg       0.96      0.95      0.95 51030000.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "eval_seg_patchcore.print_report(classes=[0, 255])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9hIuOubP3Zx"
      },
      "outputs": [],
      "source": [
        "session.view = mapped_view.shuffle().view()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1sOiF3mdkSGVWOjdF6Utiq32Way3cQtOy\">"
      ],
      "metadata": {
        "id": "QvT3foMhRw43"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VZHMTTnP3Zx"
      },
      "source": [
        "The metrics back up what we see in the app: PatchCore has much higher recall for the \"anomaly\" class, but lower precision. This means it's more likely to catch anomalies, but also more likely to make false positive predictions. After all, PatchCore is designed for \"total recall\" in industrial anomaly detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWIG5owTP3Zx"
      },
      "source": [
        "Looking at the heatmaps, we can also see what types of anomalies each model is better at detecting. An ensemble of the two models might be more robust to different types of anomalies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ku3XNWP3Zx"
      },
      "source": [
        "## Testing Data Augmentation Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5Tu6NwQP3Zy"
      },
      "source": [
        "Beyond the anomaly detection algorithm itself, there are many other knobs we can turn to improve the performance of our model. These include:\n",
        "\n",
        "- Backbone: The architecture of the model used for feature extraction\n",
        "- Algorithm hyperparameters: Parameters specific to the anomaly detection algorithm. For PatchCore, this includes `coreset_sampling_ratio` and `num_neighbors`.\n",
        "- Data augmentation: Techniques to artificially increase the size of the training set and improve the model's generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz_Ruw1dP3Zy"
      },
      "source": [
        "This section briefly illustrates the role of data augmentation techniques in anomaly detection. We'll use [FiftyOne's Albumentations plugin](https://github.com/jacobmarks/fiftyone-albumentations-plugin) to test and visualize transformations on \"good\" samples from the dataset and then apply the same transformations (with `torchvision.transforms`) to the training images.\n",
        "\n",
        "The goal is to increase the diversity of the training set without changing the images so much that we verge into the \"anomalous\" territory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcaBx2FnP3Zy"
      },
      "source": [
        "First, let's install [Albumentations](https://albumentations.ai/docs/) and download the plugin:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx7yQMALP3Zy"
      },
      "outputs": [],
      "source": [
        "pip install -U albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8cksIUZP3Zy"
      },
      "outputs": [],
      "source": [
        "!fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmSb6ezLP3Zy"
      },
      "source": [
        "Refresh the FiftyOne App and hit the backtick key on your keyboard to open the operators list. Select `\"augment_with_albumentations\"` from the dropdown menu. Try applying transformations like `GaussianBlur`, and `ColorJitter`, and see how they affect the images. Use the `\"view_last_albumentations_run\"` operator to see the augmentations generated by the last run. Play around with the kernel size for blurring, and the brightness, contrast, and saturation values for color jittering. Depending on the object category, it might also make sense to apply 90 degree rotations, horizontal flips, and other transformations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS8rjbBBP3Zy"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1UcNSQebeHw3lDnuVbmkamRu46dHIq-he\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJU9osVDP3Zy"
      },
      "source": [
        "Once you're happy with the transformations, use the `\"get_last_albumentations_run_info\"` operator to see the transformations applied and their parameters. You can then use these with `torchvision.transforms` to augment the training images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8-hsuXGP3Zy"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.v2 import GaussianBlur, ColorJitter, Compose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPpFFxe2P3Zy"
      },
      "outputs": [],
      "source": [
        "transform = Compose([\n",
        "    Resize(IMAGE_SIZE, antialias=True),\n",
        "    GaussianBlur(kernel_size=3),\n",
        "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id791HWmP3Zy"
      },
      "outputs": [],
      "source": [
        "model = Patchcore()\n",
        "augmented_inferencer = train_and_export_model(OBJECT, model, transform=transform)\n",
        "\n",
        "run_inference(mapped_view, augmented_inferencer, \"patchcore_augmented\")\n",
        "\n",
        "session.view = mapped_view.view()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxq37XEnP3Zy"
      },
      "source": [
        "This is just a starting point for data augmentation. You can experiment with different transformations and parameters to see what works best for your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VK6LqpxP3Zy"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50xh-dv6P3Zy"
      },
      "source": [
        "In this walkthrough, we learned how to perform anomaly detection on visual data using FiftyOne and Anomalib. We trained two anomaly detection models, PaDiM and PatchCore, on the MVTec AD dataset and evaluated their performance using metrics like precision, recall, and F1 score. We also visualized the models' predictions using heatmaps and masks. Finally, we tested data augmentation techniques to improve the models' generalization.\n",
        "\n",
        "If you want to dive deeper into unsupervised learning, check out these tutorials:\n",
        "\n",
        "- [Visualizing Data with Dimensionality Reduction Techniques](https://docs.voxel51.com/tutorials/dimension_reduction.html)\n",
        "- [Clustering Images with Embeddings](https://docs.voxel51.com/tutorials/clustering.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kRfc8PbP3Zz"
      },
      "outputs": [],
      "source": [
        "session.view = mapped_view.view()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FiTZmhjP3Zz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "anomalib_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}